# -*- coding: utf-8 -*-
"""model_training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rDBmL_xc1UHFoeHLMgyldQDAWSpKDJ9M
"""

## Import gogle drive and Mount the Google drive for getting Data

!pip install optuna

from sentence_transformers import SentenceTransformer
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score , roc_auc_score
from xgboost import XGBClassifier
from xgboost import plot_importance
# import optuna
import statsmodels.api as sm

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# Load pre-trained embedding model
model = SentenceTransformer('all-MiniLM-L6-v2')

def compute_alignment_features(resume_text, jd_text, transcript_text):
    features = {}

    # Embedding-based similarity
    resume_embedding = model.encode([resume_text])[0]
    jd_embedding = model.encode([jd_text])[0]
    transcript_embedding = model.encode([transcript_text])[0]

    features['resume_jd_similarity'] = cosine_similarity([resume_embedding], [jd_embedding])[0][0]
    features['transcript_jd_similarity'] = cosine_similarity([transcript_embedding], [jd_embedding])[0][0]

    return features

data = pd.read_excel('/content/drive/MyDrive/final_data.xlsx')

data.info()

# Sentiment analysis using TestBlob library

from textblob import TextBlob

def analyze_sentiment(text):
    # Create a TextBlob object
    blob = TextBlob(text)

    # Get the sentiment polarity
    polarity = blob.sentiment.polarity

    return polarity

data['Transcript_sentiment'] = data['Transcript_processed'].apply(analyze_sentiment)
data['Resume_sentiment'] = data['Resume_processed'].apply(analyze_sentiment)
data['JobDescription_sentiment'] = data['Job_Description_processed'].apply(analyze_sentiment)

# Word count feature

def word_counts(text):
    return len(text.split())

data['Transcript_words'] = data['Transcript_processed'].apply(word_counts)
data['Resume_words'] = data['Resume_processed'].apply(word_counts)

# Extracting years of experience

import re
from datetime import datetime

def extract_years_of_experience(text):
    # Regular expression to extract years and date ranges
    date_ranges = re.findall(r"(\d{4})-(present|\d{4})", text.lower())

    total_years = 0
    current_year = datetime.now().year

    for start, end in date_ranges:
        start_year = int(start)
        end_year = current_year if end == "present" else int(end)

        total_years += (end_year - start_year)

    return total_years
data['Years_Experience']=data['Resume'].apply(extract_years_of_experience)

# Function for calculating Tf-Idf vectorizer embeddings and then similarity



import joblib
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def compute_tfidf_cosine_similarity(data, col1, col2, save_model=True):
    """
    Computes cosine similarity between resumes and job descriptions using TF-IDF,
    fits the vectorizer once, and optionally saves the model.

    Parameters:
    - data (pd.DataFrame): Dataset containing resumes and job descriptions.
    - col1 (str): Column name for resumes.
    - col2 (str): Column name for job descriptions.
    - save_model (bool): Whether to save the fitted TfidfVectorizer model.

    Returns:
    - similarities (list): List of cosine similarity scores for each resume-JD pair.
    """
    # Combine all text data from both columns to fit the vectorizer once
    combined_texts = data[col1].tolist() + data[col2].tolist()

    # Fit the TF-IDF vectorizer on the combined texts
    tfidf_vectorizer = TfidfVectorizer(stop_words='english')
    tfidf_vectorizer.fit(combined_texts)

    # Optionally save the model
    if save_model:
        model_filename = f"sim_of_{col1}_and_{col2}.joblib"
        joblib.dump(tfidf_vectorizer, model_filename)
        print(f"TF-IDF model saved as: {model_filename}")

    # Transform each column's text into TF-IDF vectors
    tfidf_resumes = tfidf_vectorizer.transform(data[col1])
    tfidf_jds = tfidf_vectorizer.transform(data[col2])

    # Compute cosine similarity for each pair of vectors
    similarities = [
        cosine_similarity(tfidf_resumes[i], tfidf_jds[i])[0][0]
        for i in range(len(data))
    ]

    return similarities

data['resume_jd_similarity'] = compute_tfidf_cosine_similarity(data , 'Resume_processed' , 'Job_Description_processed')

import joblib

# Load the saved TF-IDF model
model_filename = "/content/sim_of_Resume_processed_and_Job_Description_processed.joblib"
tfidf_vectorizer = joblib.load(model_filename)

print(f"Loaded TF-IDF model from: {model_filename}")

# Transform new text data
resume_vector = tfidf_vectorizer.transform(data['Resume_processed'][0:2]).toarray()
jd_vector = tfidf_vectorizer.transform(data['Job_Description_processed'][0:2]).toarray()

# # Compute cosine similarity
# similarity = cosine_similarity(resume_vector, jd_vector)
# print("Cosine Similarity:", similarity[0][0])

for i in range(len(resume_vector)):
  print(cosine_similarity(resume_vector[i].reshape(1, -1), jd_vector[i].reshape(1, -1))))

similarities = [
    np.round(cosine_similarity(resume_vector[i].reshape(1, -1), jd_vector[i].reshape(1, -1))[0][0],6)
    for i in range(len(resume_vector))
]

similarities

data['Transcript_jd_similarity'] = compute_tfidf_cosine_similarity(data , 'Transcript_processed' , 'Job_Description_processed')

data['resume_transcript_similarity'] = compute_tfidf_cosine_similarity(data , 'Resume_processed' , 'Transcript_processed')

import nltk
nltk.download('punkt_tab')

import re
from nltk.tokenize import word_tokenize, sent_tokenize

## Finding Resume and Job description based features like , skill match count and no.of education degrees from b.tech on wards , etc..

def extract_resume_features(resume_text, jd_text):
    features = {}

    # Length-based features
    features['resume_sentence_count'] = len(sent_tokenize(resume_text))
    features['resume_avg_word_length'] = sum(len(word) for word in word_tokenize(resume_text)) / len(word_tokenize(resume_text))

    # Skill match features
    jd_skills = set(re.findall(r'\b[A-Za-z]+\b', jd_text.lower()))
    resume_skills = set(re.findall(r'\b[A-Za-z]+\b', resume_text.lower()))
    features['skill_match_count'] = len(jd_skills & resume_skills)

    # Education features
    degrees = ['b.tech', 'm.tech', 'mba', 'phd', 'bachelor', 'master']
    features['university_education_count'] = sum(1 for degree in degrees if degree in resume_text.lower())

    return features

def process_dataset(data, resume_col, jd_col):
    """
    Process a dataset to extract features for each resume-JD pair.

    Parameters:
    - data (pd.DataFrame): Dataset containing resumes and job descriptions.
    - resume_col (str): Column name for resumes.
    - jd_col (str): Column name for job descriptions.

    Returns:
    - pd.DataFrame: Original dataset with extracted features appended.
    """
    extracted_features = []

    for _, row in data.iterrows():
        resume_text = row[resume_col]
        jd_text = row[jd_col]
        features = extract_resume_features(resume_text, jd_text)
        extracted_features.append(features)

    # Combine features into a DataFrame
    features_df = pd.DataFrame(extracted_features)
    return pd.concat([data, features_df], axis=1)

# Process the dataset
processed_data = process_dataset(data, "Resume_processed", "Job_Description_processed")

processed_data.columns

## Extracting the features from transcript like vocabulary diversity , avg_sentence_length

def extract_transcript_features(transcript_text):
    features = {}
    # Language features
    features['transcript_vocab_diversity'] = len(set(word_tokenize(transcript_text))) / len(word_tokenize(transcript_text))
    features['transcript_avg_sentence_length'] = sum(len(sent.split()) for sent in sent_tokenize(transcript_text)) / len(sent_tokenize(transcript_text))
    return features

def process_transcript(data, transcript_col = 'Transcript_processed'):
    extracted_features = []

    for _, row in data.iterrows():
        transcript_text = row[transcript_col]
        features = extract_transcript_features(transcript_text)
        extracted_features.append(features)

    # Combine features into a DataFrame
    features_df = pd.DataFrame(extracted_features)
    return pd.concat([data, features_df], axis=1)

# Process the dataset
processed_data = process_transcript(processed_data)

processed_data.columns

processed_data.to_excel('final_data_features.xlsx')

"""# ***Building Complete models using Tf-Idf embeddings similarity:***"""

import warnings

# Ignore all warnings
warnings.filterwarnings("ignore")

req_data = processed_data[['Transcript_sentiment',
       'Resume_sentiment', 'JobDescription_sentiment', 'Transcript_words',
       'Resume_words', 'Years_Experience', 'resume_jd_similarity',
       'Transcript_jd_similarity', 'resume_transcript_similarity',
       'resume_sentence_count', 'resume_avg_word_length', 'skill_match_count',
       'university_education_count', 'transcript_vocab_diversity',
       'transcript_avg_sentence_length' , 'decision']]

req_data['decision_processed'] = req_data['decision'].apply(lambda x : 1 if x == 'select' else 0)

req_data.head()

"""1. Splitting the data for training and testing  """

x , y = req_data.drop(columns = ['decision' , 'decision_processed']) , req_data['decision_processed']

train_x , test_x , train_y , test_y = train_test_split(x,y,train_size = 0.8 , random_state = 42)

train_x.info()

train_y.head()

# pip install optuna

## Optuna is used for hyper parameter tuning

## Addding the minor noise in order to eliminate perfect correlation for logistic regression implementation in statsmodels library
train_x += np.random.normal(0, 1e-4, train_x.shape)
test_x += np.random.normal(0, 1e-4, test_x.shape)

"""Logistic regression model building and analysis"""

# Logistic Regression using statsmodels
def logistic_regression_statsmodels(train_x, train_y):
    X_train_const = sm.add_constant(train_x)  # Add intercept
    model = sm.Logit(train_y, X_train_const)
    result = model.fit()
    print(result.summary())
    return result

# Logistic Regression Model
logistic_model = logistic_regression_statsmodels(train_x, train_y)
X_test_const = sm.add_constant(test_x)
y_pred_logistic = (logistic_model.predict(X_test_const) > 0.5).astype(int)

# Logistic Regression Evaluation
print("Logistic Regression Metrics:")
print("Accuracy:", accuracy_score(test_y, y_pred_logistic))
print("Precision:", precision_score(test_y, y_pred_logistic))
print("Recall:", recall_score(test_y, y_pred_logistic))
print("F1 Score:", f1_score(test_y, y_pred_logistic))
print("roc_auc Score:", roc_auc_score(test_y, y_pred_logistic))

"""1. from this model it came to know that Transcript words , resume_job desc similarity are not much useful
2. resume transcript similarity , transcript avg sentence length , resume sent count , university count , resume avg words are also not uyseful

This might be due to Tf-idf transformer

"""

# model without those cols ,
x1 , y1 = req_data.drop(columns = ['decision' , 'decision_processed' , 'Transcript_words' , 'resume_jd_similarity' , 'resume_transcript_similarity' , 'resume_sentence_count' , 'university_education_count' , 'resume_avg_word_length' , 'transcript_avg_sentence_length' , 'Resume_words' , 'Years_Experience']) , req_data['decision_processed']

train_x1 , test_x1 , train_y1 , test_y1 = train_test_split(x1,y1,train_size = 0.8 , random_state = 42)

# Logistic Regression using statsmodels
def logistic_regression_statsmodels(train_x, train_y):
    X_train_const = sm.add_constant(train_x)  # Add intercept
    model = sm.Logit(train_y, X_train_const)
    result = model.fit()
    print(result.summary())
    return result

# Logistic Regression Model
logistic_model = logistic_regression_statsmodels(train_x1, train_y1)
X_test_const = sm.add_constant(test_x1)
y_pred_logistic = (logistic_model.predict(X_test_const) > 0.5).astype(int)

# Logistic Regression Evaluation
print("Logistic Regression Metrics:")
print("Accuracy:", accuracy_score(test_y1, y_pred_logistic))
print("Precision:", precision_score(test_y1, y_pred_logistic))
print("Recall:", recall_score(test_y1, y_pred_logistic))
print("F1 Score:", f1_score(test_y1, y_pred_logistic))
print("roc_auc Score:", roc_auc_score(test_y1, y_pred_logistic))

# XGBoost with Optuna for Hyperparameter Tuning
def objective(trial):
    params = {
        "n_estimators": trial.suggest_int("n_estimators", 100, 1000),
        "max_depth": trial.suggest_int("max_depth", 3, 10),
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
        "subsample": trial.suggest_float("subsample", 0.5, 1.0),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.5, 1.0),
        "gamma": trial.suggest_float("gamma", 0, 5),
        "min_child_weight": trial.suggest_int("min_child_weight", 1, 10),
    }

    model = XGBClassifier(**params, use_label_encoder=False, eval_metric="logloss", random_state=42)
    model.fit(train_x, train_y)
    y_pred = model.predict(test_x)
    return accuracy_score(test_y, y_pred)

study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=50)

# Best hyperparameters
print("Best parameters:", study.best_params)

# Train XGBoost with best parameters
best_params = study.best_params
xgb_model = XGBClassifier(**best_params, use_label_encoder=False, eval_metric="logloss", random_state=42)
xgb_model.fit(train_x, train_y)
y_pred_xgb = xgb_model.predict(test_x)

# XGBoost Evaluation
print("XGBoost Metrics:")
print("Accuracy:", accuracy_score(test_y, y_pred_xgb))
print("Precision:", precision_score(test_y, y_pred_xgb))
print("Recall:", recall_score(test_y, y_pred_xgb))
print("F1 Score:", f1_score(test_y, y_pred_xgb))

## Plotting the feature importance for Xgboost model
plt.figure(figsize=(10, 8))
plot_importance(xgb_model, importance_type='weight', title='Feature Importance')
plt.show()

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Train and evaluate Naive Bayes
nb_model = GaussianNB()
nb_model.fit(train_x, train_y)  # Fit the model
y_pred_nb = nb_model.predict(test_x)  # Predict on test set

# Evaluation
print("Naive Bayes Metrics (TF-IDF):")
print("Accuracy:", accuracy_score(test_y, y_pred_nb))
print("Precision:", precision_score(test_y, y_pred_nb))
print("Recall:", recall_score(test_y, y_pred_nb))
print("F1 Score:", f1_score(test_y, y_pred_nb))

from sklearn.ensemble import RandomForestClassifier

# Train and evaluate Random Forest Classifier
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(train_x, train_y)  # Fit the model
y_pred_rf = rf_model.predict(test_x)  # Predict on test set

# Evaluation
print("Random Forest Metrics (TF-IDF):")
print("Accuracy:", accuracy_score(test_y, y_pred_rf))
print("Precision:", precision_score(test_y, y_pred_rf))
print("Recall:", recall_score(test_y, y_pred_rf))
print("F1 Score:", f1_score(test_y, y_pred_rf))

# Add values on top of the bars
importance = rf_model.feature_importances_


plt.figure(figsize=(10, 6))
plt.bar(train_x.columns, importance)
plt.xticks(rotation=90)
plt.title("Random Forest Feature Importance")

# Annotate each bar with its value
for i, v in enumerate(importance):
    plt.text(i, v + 0.001, f"{v:.2f}", ha='center', va='bottom', fontsize=8)

plt.show()

from sklearn.svm import SVC

# Train and evaluate SVC
svc_model = SVC(kernel="linear", random_state=42)
svc_model.fit(train_x, train_y)  # Fit the model
y_pred_svc = svc_model.predict(test_x)  # Predict on test set

# Evaluation
print("SVC Metrics (TF-IDF):")
print("Accuracy:", accuracy_score(test_y, y_pred_svc))
print("Precision:", precision_score(test_y, y_pred_svc))
print("Recall:", recall_score(test_y, y_pred_svc))
print("F1 Score:", f1_score(test_y, y_pred_svc))

"""# ***Computation of Similarity scores between resume , transcript , job description using Sentence Transformers Library***"""

model = SentenceTransformer('all-MiniLM-L6-v2')

def compute_alignment_features(resume_text, jd_text, transcript_text):
    features = {}

    """Embedding-based similarity"""
    resume_embedding = model.encode([resume_text])[0]
    jd_embedding = model.encode([jd_text])[0]
    transcript_embedding = model.encode([transcript_text])[0]

    features['resume_jd_similarity_transformers'] = cosine_similarity([resume_embedding], [jd_embedding])[0][0]
    features['transcript_jd_similarity_transformers'] = cosine_similarity([transcript_embedding], [jd_embedding])[0][0]
    features['transcript_resume_similarity_transformers'] = cosine_similarity([transcript_embedding], [resume_embedding])[0][0]

    return features

def process_transformer_similarity(data):
    extracted_features = []

    for _, row in data.iterrows():
        resume_text = row['Resume_processed']
        jd_text = row['Job_Description_processed']
        transcript_text = row['Transcript_processed']
        features = compute_alignment_features(resume_text, jd_text, transcript_text)
        extracted_features.append(features)

    # Combine features into a DataFrame
    features_df = pd.DataFrame(extracted_features)
    return pd.concat([data, features_df], axis=1)

# Process the dataset
processed_data = process_transformer_similarity(processed_data)

processed_data.columns

processed_data = pd.read_excel('/content/processed_data.xlsx')

req_data = processed_data[['Transcript_sentiment',
       'Resume_sentiment', 'JobDescription_sentiment', 'Transcript_words',
       'Resume_words', 'Years_Experience', 'resume_jd_similarity_transformers',
       'transcript_jd_similarity_transformers',
       'transcript_resume_similarity_transformers',
       'resume_sentence_count', 'resume_avg_word_length', 'skill_match_count',
       'university_education_count', 'transcript_vocab_diversity',
       'transcript_avg_sentence_length' , 'decision']]
req_data['decision_processed'] = req_data['decision'].apply(lambda x : 1 if x == 'select' else 0)
x , y = req_data.drop(columns = ['decision' , 'decision_processed']) , req_data['decision_processed']
train_x , test_x , train_y , test_y = train_test_split(x,y,train_size = 0.8 , random_state = 42)

train_x += np.random.normal(0, 1e-4, train_x.shape)
test_x += np.random.normal(0, 1e-4, test_x.shape)

"""# ***Building Complete models using sentence Transformer embeddings similarity:***"""

# Logistic Regression using statsmodels
def logistic_regression_statsmodels(train_x, train_y):
    X_train_const = sm.add_constant(train_x)  # Add intercept
    model = sm.Logit(train_y, X_train_const)
    result = model.fit()
    print(result.summary())
    return result

# Logistic Regression Model
logistic_model = logistic_regression_statsmodels(train_x, train_y)
X_test_const = sm.add_constant(test_x)
y_pred_logistic = (logistic_model.predict(X_test_const) > 0.5).astype(int)

# Logistic Regression Evaluation
print("Logistic Regression Metrics:")
print("Accuracy:", accuracy_score(test_y, y_pred_logistic))
print("Precision:", precision_score(test_y, y_pred_logistic))
print("Recall:", recall_score(test_y, y_pred_logistic))
print("F1 Score:", f1_score(test_y, y_pred_logistic))
print("roc_auc Score:", roc_auc_score(test_y, y_pred_logistic))

"""After using the Sentence transformers for finding similarity we got that
resume jd similarity and resume transcript similarity as useful features
"""

# XGBoost with Optuna for Hyperparameter Tuning
def objective(trial):
    params = {
        "n_estimators": trial.suggest_int("n_estimators", 100, 1000),
        "max_depth": trial.suggest_int("max_depth", 3, 10),
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
        "subsample": trial.suggest_float("subsample", 0.5, 1.0),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.5, 1.0),
        "gamma": trial.suggest_float("gamma", 0, 5),
        "min_child_weight": trial.suggest_int("min_child_weight", 1, 10),
    }

    model = XGBClassifier(**params, use_label_encoder=False, eval_metric="logloss", random_state=42)
    model.fit(train_x, train_y)
    y_pred = model.predict(test_x)
    return accuracy_score(test_y, y_pred)

study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=50)

# Best hyperparameters
print("Best parameters:", study.best_params)

# Train XGBoost with best parameters
best_params = study.best_params
xgb_model = XGBClassifier(**best_params, use_label_encoder=False, eval_metric="logloss", random_state=42)
xgb_model.fit(train_x, train_y)
y_pred_xgb = xgb_model.predict(test_x)

# XGBoost Evaluation
print("XGBoost Metrics:")
print("Accuracy:", accuracy_score(test_y, y_pred_xgb))
print("Precision:", precision_score(test_y, y_pred_xgb))
print("Recall:", recall_score(test_y, y_pred_xgb))
print("F1 Score:", f1_score(test_y, y_pred_xgb))

plt.figure(figsize=(10, 8))
plot_importance(xgb_model, importance_type='weight', title='Feature Importance')
plt.show()

# Train and evaluate Naive Bayes
nb_model2 = GaussianNB()
nb_model2.fit(train_x, train_y)  # Fit the model
y_pred_nb = nb_model2.predict(test_x)  # Predict on test set

# Evaluation
print("Naive Bayes Metrics (SentenceTransformer):")
print("Accuracy:", accuracy_score(test_y, y_pred_nb))
print("Precision:", precision_score(test_y, y_pred_nb))
print("Recall:", recall_score(test_y, y_pred_nb))
print("F1 Score:", f1_score(test_y, y_pred_nb))

importance = 1 / np.sqrt(nb_model2.var_)
mean_importance = importance.mean(axis=0)  # Average across classes
print("Mean Feature Importance:", mean_importance)

# Train and evaluate Random Forest Classifier
rf_model2 = RandomForestClassifier(random_state=42)
rf_model2.fit(train_x, train_y)  # Fit the model
y_pred_rf = rf_model2.predict(test_x)  # Predict on test set

# Evaluation
print("Random Forest Metrics (SentenceTransformer):")
print("Accuracy:", accuracy_score(test_y, y_pred_rf))
print("Precision:", precision_score(test_y, y_pred_rf))
print("Recall:", recall_score(test_y, y_pred_rf))
print("F1 Score:", f1_score(test_y, y_pred_rf))

importance = rf_model2.feature_importances_
plt.figure(figsize=(10, 6))
plt.bar(x.columns, importance)
plt.xticks(rotation=90)
plt.title("Random Forest Feature Importance")
for i, v in enumerate(importance):
    plt.text(i, v + 0.001, f"{v:.2f}", ha='center', va='bottom', fontsize=8)

plt.show()

# Train and evaluate SVC
svc_model = SVC(kernel="linear", random_state=42)
svc_model.fit(train_x, train_y)  # Fit the model
y_pred_svc = svc_model.predict(test_x)  # Predict on test set

# Evaluation
print("SVC Metrics (SentenceTransformer):")
print("Accuracy:", accuracy_score(test_y, y_pred_svc))
print("Precision:", precision_score(test_y, y_pred_svc))
print("Recall:", recall_score(test_y, y_pred_svc))
print("F1 Score:", f1_score(test_y, y_pred_svc))

"""# ***In comparision to all the models, ***

# ***XGBOOST model is working well with the data as it is giving high accuracy , with sentence transformers ***
"""

# so saving the best model

y_pred_xgb = xgb_model.predict(test_x)

# XGBoost Evaluation
print("XGBoost Metrics:")
print("Accuracy:", accuracy_score(test_y, y_pred_xgb))
print("Precision:", precision_score(test_y, y_pred_xgb))
print("Recall:", recall_score(test_y, y_pred_xgb))
print("F1 Score:", f1_score(test_y, y_pred_xgb))

import joblib

# Save the model
joblib.dump(model, 'xgb_classifier_model.pkl')

import joblib

# Load the model
model = joblib.load('xgb_classifier_model.pkl')

## Post Processing after model building

import shap
import numpy as np
import matplotlib.pyplot as plt

# Initialize SHAP explainer
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(test_x)

"""## **Feature Importance using Beswarm Plot**

#### 1. **Transcript_words**
##### Observation: A higher number of words in the transcript (red points) increases the prediction probability, while fewer words (blue points) decrease it.
##### Impact: Highly positive feature, indicating that longer transcripts are associated with better outcomes.

#### 2. **transcript_avg_sentence_length**
##### Observation: Longer average sentence lengths (red points) tend to slightly increase prediction probabilities. Shorter lengths (blue points) reduce them.
##### Impact: Moderate positive influence, suggesting coherence or richness in sentence structure contributes positively.

#### 3. **JobDescription_sentiment**
##### Observation: Positive sentiment in job descriptions (red points) slightly improves the prediction. Negative sentiment (blue points) reduces it.
##### Impact: A small but meaningful role in aligning candidate attributes with the tone of the job description.

#### 4. **Resume_sentiment**
##### Observation: Resumes with positive sentiment (red points) improve prediction probability, while those with negative sentiment (blue points) lower it.
##### Impact: Moderate positive influence, showing that optimistic or enthusiastic resumes may be more impactful.

#### 5. **university_education_count**
##### Observation: Higher counts (e.g., multiple degrees or certifications, red points) improve prediction. Lower counts (blue points) decrease probability.
##### Impact: Strong positive feature, suggesting higher educational credentials positively influence predictions.

#### 6. **transcript_vocab_diversity**
##### Observation: Higher vocabulary diversity (red points) is associated with higher prediction probabilities. Lower diversity (blue points) reduces it.
##### Impact: Indicates that varied and sophisticated vocabulary in transcripts is advantageous.

#### 7. **resume_jd_similarity_transformers**
##### Observation: Higher similarity scores between the resume and job description (red points) improve prediction probabilities. Lower scores (blue points) reduce them.
##### Impact: Critical feature, suggesting alignment between a candidate's resume and job requirements is vital.

#### 8. **Resume_words**
##### Observation: A higher number of words in resumes (red points) slightly increases prediction probability. Fewer words (blue points) decrease it.
##### Impact: Moderate feature, indicating detailed resumes may perform better.

#### 9. **Years_Experience**
##### Observation: More years of experience (red points) increase prediction probabilities, while fewer years (blue points) decrease them.
##### Impact: Strong positive influence, as experience correlates with job suitability.

#### 10. **transcript_jd_similarity_transformers**
##### Observation: Higher similarity between the transcript and job description (red points) improves predictions, while lower similarity (blue points) decreases them.
##### Impact: Significant feature, emphasizing the importance of aligning communication (e.g., during interviews) with job requirements.

#### 11. **Transcript_sentiment**
##### Observation: Positive sentiment in the transcript (red points) increases prediction probabilities. Negative sentiment (blue points) decreases them.
##### Impact: Moderate role, showing that a positive tone during communication is beneficial.

#### 12. **resume_sentence_count**
##### Observation: Resumes with more sentences (red points) increase prediction probabilities. Fewer sentences (blue points) reduce them.
##### Impact: Indicates that detailed resumes with clear segmentation are advantageous.

#### 13. **resume_avg_word_length**
##### Observation: Longer average word lengths (red points) tend to increase prediction probabilities. Shorter words (blue points) decrease them.
##### Impact: Indicates that using more sophisticated vocabulary positively influences predictions.

#### 14. **skill_match_count**
##### Observation: Higher skill matches (red points) significantly increase prediction probabilities. Fewer matches (blue points) decrease them.
##### Impact: Critical feature, as skill alignment is fundamental to job success.

#### 15. **transcript_resume_similarity_transformers**
##### Observation: Higher similarity between the transcript and resume (red points) improves predictions. Lower similarity (blue points) reduces them.
##### Impact: Important feature, emphasizing consistency between interview communication and the resume.

"""

# Beeswarm plot
shap.summary_plot(shap_values, test_x)

"""# **Waterfall Plot**"""

# Finding the Lowest , middle , high probability predictions from the model

low_index = np.argmin(y_pred_xgb)  # Lowest prediction
medium_index = np.argsort(y_pred_xgb)[len(y_pred_xgb) // 2]  # Median prediction
high_index = np.argmax(y_pred_xgb)  # Highest prediction

"""### **Low prediction**

1. **For low predictions from this water fall plot we can observe that the number of words in transcript are contributing highly negitive  in the prediction of probability for success in interview**

2. **Even the transcript average sentence length
 , resume job description similarity is contributing highly negitive  in the prediction of probability for select in interview**

3. **Transcript job description similarity and  resume sentiment are the only 2 features which are contributing positively but they are also contributing very slightly for increase in the prediction**
"""

# Waterfall plots
shap.waterfall_plot(shap.Explanation(values=shap_values[low_index],
                                     base_values=explainer.expected_value,
                                     data=test_x.iloc[low_index]),
                    max_display=10)

"""### **Medium Prediction**

1. **For Medium predictions from this water fall plot we can observe that the number of words in transcript are contributing highly positive(+2.01)  in the prediction of probability for success in interview**

2. **Even the transcript average sentence length
 , resume sentiment is contributing highly positive in the prediction of probability for select in interview**

3. **Transcript vocabulary diversity , resume job descripition similarity using transformers are the ones whuch are contributing negitively  but they are also slightly negitive**
"""

shap.waterfall_plot(shap.Explanation(values=shap_values[medium_index],
                                     base_values=explainer.expected_value,
                                     data=test_x.iloc[medium_index]),
                    max_display=10)

"""### **High prediction**

1. **For higher predictions from this water fall plot we can observe that the number of words in transcript are contributing highly positive(+2.7) even compared for medium predictions, in the prediction of probability for success in interview**

2. **Even the transcript average sentence length
 , transcript job description similarity , resume sentiment is contributing highly positive in the prediction of probability for select in interview**

3. **Transcript vocabulary diversity is the only feature whuch is contributing negitively  but that contribution is also very slightly (-0.45)**
"""

shap.waterfall_plot(shap.Explanation(values=shap_values[high_index],
                                     base_values=explainer.expected_value,
                                     data=test_x.iloc[high_index]),
                    max_display=10)

from sklearn.inspection import PartialDependenceDisplay

# Verify the feature alignment
expected_features = model.feature_names_in_  # Features used in training
train_x = train_x[expected_features]  # Align the features

# Specify features and target class
features_to_plot = ['Transcript_words']  # Indices of the features
target_class = 1  # Specify target class for multi-class PDP

# Generate Partial Dependence Plot
disp = PartialDependenceDisplay.from_estimator(
    xgb_model,
    train_x,
    features=features_to_plot,
    target=target_class,
    feature_names=expected_features,
    kind="average",
    grid_resolution=50
)

plt.suptitle(f"1D Partial Dependence Plots for Class {target_class}")
plt.show()

# Verify the feature alignment
expected_features = model.feature_names_in_  # Features used in training
train_x = train_x[expected_features]  # Align the features

# Specify features and target class
features_to_plot = ['transcript_avg_sentence_length']  # Indices of the features
target_class = 1  # Specify target class for multi-class PDP

# Generate Partial Dependence Plot
disp = PartialDependenceDisplay.from_estimator(
    xgb_model,
    train_x,
    features=features_to_plot,
    target=target_class,
    feature_names=expected_features,
    kind="average",
    grid_resolution=50
)

plt.suptitle(f"1D Partial Dependence Plots for Class {target_class}")
plt.show()

# Verify the feature alignment
expected_features = model.feature_names_in_  # Features used in training
train_x = train_x[expected_features]  # Align the features

# Specify features and target class
features_to_plot = ['transcript_vocab_diversity']  # Indices of the features
target_class = 1  # Specify target class for multi-class PDP

# Generate Partial Dependence Plot
disp = PartialDependenceDisplay.from_estimator(
    xgb_model,
    train_x,
    features=features_to_plot,
    target=target_class,
    feature_names=expected_features,
    kind="average",
    grid_resolution=50
)

plt.suptitle(f"1D Partial Dependence Plots for Class {target_class}")
plt.show()

# Verify the feature alignment
expected_features = model.feature_names_in_  # Features used in training
train_x = train_x[expected_features]  # Align the features

# Specify features and target class
features_to_plot = ['skill_match_count']  # Indices of the features
target_class = 1  # Specify target class for multi-class PDP

# Generate Partial Dependence Plot
disp = PartialDependenceDisplay.from_estimator(
    xgb_model,
    train_x,
    features=features_to_plot,
    target=target_class,
    feature_names=expected_features,
    kind="average",
    grid_resolution=50
)

plt.suptitle(f"1D Partial Dependence Plots for Class {target_class}")
plt.show()

import pickle

with open("/content/xgb_classifier_model.pkl", "rb") as file:
    model = pickle.load(file)

# Assuming the model is trained for classification
model.classes_ = np.unique(train_y)

setattr(model, "use_label_encoder", False)  # Restore the missing attribute

# After loading the model
setattr(model, 'gpu_id', -1)  # Set to -1 if using CPU, or set appropriate value for GPU

from xgboost import XGBClassifier
import xgboost as xgb

# After loading the model
if hasattr(model, 'predictor') is False:
    model._Booster = xgb.Booster(model_file='/content/xgb_model.json')  # Manually set the Booster
    model.predictor = model._Booster  # Set the predictor attribute explicitly

# Verify the feature alignment
expected_features = model.feature_names_in_  # Features used in training
train_x = train_x[expected_features]  # Align the features

# Specify features and target class
features_to_plot = [('transcript_vocab_diversity' , 'Transcript_words')]  # Indices of the features
target_class = 1  # Specify target class for multi-class PDP

# Generate Partial Dependence Plot
disp = PartialDependenceDisplay.from_estimator(
    xgb_model,
    train_x,
    features=features_to_plot,
    target=target_class,
    feature_names=expected_features,
    kind="average",
    grid_resolution=50
)

plt.suptitle(f"2D Partial Dependence Plots for Class {target_class}")
plt.show()

# Verify the feature alignment
expected_features = model.feature_names_in_  # Features used in training
train_x = train_x[expected_features]  # Align the features

# Specify features and target class
features_to_plot = ['skill_match_count']  # Indices of the features
target_class = 1  # Specify target class for multi-class PDP

# Generate Partial Dependence Plot
disp = PartialDependenceDisplay.from_estimator(
    model,
    train_x,
    features=features_to_plot,
    target=target_class,
    feature_names=expected_features,
    kind="average",
    grid_resolution=50
)

plt.suptitle(f"1D Partial Dependence Plots for Class {target_class}")
plt.show()







