# -*- coding: utf-8 -*-
"""dataCreation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/131msdW4logfbOWyO6P2namxcGfpd7ile
"""

# !pip install together

from together import Together
import os

"""# *** Creating Different Roles and performance in interview and reason for decision taken for generating data ***"""

roles = ['Data Scientist' , 'Data Analyst' , 'Data Engineer' , 'Software Engineer' , 'Machine Learning Engineer']
performance = ['select','reject']
rejection_reason = ['Insufficient technical knowledge for the required role',
                    'Lack of relevant industry experience',
                    'Poor communication or interpersonal skills',
                    'Inability to solve technical or problem-solving tasks during interviews',
                    'Misalignment with company culture or values',
                    'Inadequate understanding of job responsibilities or domain expertise',
                    'Overqualification or salary expectations beyond the budget',
                    'Failure to provide strong references or credible past work evidence',
                    'Lack of enthusiasm or motivation for the role',
                    'Limited adaptability to work in a team or dynamic environments']
selection_reasons = [
    'Demonstrated strong technical knowledge and expertise in the required domain',
    'Relevant and extensive industry experience aligned with the role',
    'Excellent communication and interpersonal skills',
    'Exceptional problem-solving abilities during technical assessments',
    'Alignment with company culture and values',
    'Clear understanding of job responsibilities and domain expertise',
    'Flexibility and adaptability to dynamic work environments',
    'Proven track record and strong references from past roles',
    'Enthusiasm and motivation to contribute to the companyâ€™s goals',
    'Effective team collaboration and leadership potential'
]

os.environ['TOGETHER_API_KEY'] = 'cd9e34c00f76d13a8b55b3d34e48f782ac6ac0529cf14bf819a6b88d38d11cd0'
client = Together()

"""# ***Using Gen AI for generating Synthetic Data***"""

## Job Description

job_descriptions = {role:[] for role in roles}
for job in roles:
  for i in range(5):
      response = client.chat.completions.create(
                          model="meta-llama/Llama-3.3-70B-Instruct-Turbo",
                          messages=[{"role": "user", "content": f"Can u generate Job description for {job} role by assuming some required details and which is not generated previously, dont generate any text other than the job description like here is the job description ?"}],)
      job_descriptions[job].append(response.choices[0].message.content)
  job_descriptions[job] = list(set(job_descriptions[job]))

print(job_descriptions['Software Engineer'][1])

import time
import pandas as pd

df = pd.DataFrame(columns=['Role', 'Transcript', 'Resume', 'Performance(select/reject)', 'Reason for decision', 'Job Description'])

df.head()

## Resume , Transcript Generation along with the data set creation

# Synthetic Data Generation

for job in roles:
    for perf in performance:
        if perf == 'reject':
            for reason in rejection_reason:
                for desc in job_descriptions[job]:
                    # Prompt for generating transcript
                    transcript_prompt = (
                        f"Can you generate the transcript between the interviewer and interviewee for the role of {job} "
                        f"such that the candidate got {perf} and the reason for rejection should be {reason} "
                        f"and the description of the job looks like {desc}"
                        "Don't generate any text apart from the required thing"
                    )
                    response = client.chat.completions.create(
                        model="meta-llama/Llama-3.3-70B-Instruct-Turbo",
                        messages=[{"role": "user", "content": transcript_prompt}],
                    )
                    transcript = response.choices[0].message.content

                    # Prompt for generating resume
                    resume_prompt = (
                        f"Can you generate a resume for a candidate applying for the role of {job} who was rejected? "
                        f"The rejection reason is {reason}, and the job description looks like: {desc}. "
                        f"The resume should reflect a typical candidate profile for this job and the rejection reason."
                        "Don't generate any text apart from the required thing"
                    )
                    resume_response = client.chat.completions.create(
                        model="meta-llama/Llama-3.3-70B-Instruct-Turbo",
                        messages=[{"role": "user", "content": resume_prompt}],
                    )
                    resume = resume_response.choices[0].message.content


                    row = {
                              'Role': job,
                              'Transcript': transcript,
                              'Resume': resume,
                              'Performance(select/reject)': perf,
                              'Reason for decision': reason,
                              'Job Description': desc
                            }

                    df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)

                    # Print and wait to prevent exceeding per-second requests
                    print(f"Processed rejection for {job} with reason: {reason}")
                    time.sleep(1)  # Sleep for 1 second to limit request rate

        else:  # For "select" performance
            for reason in selection_reasons:
                for desc in job_descriptions[job]:
                    # Prompt for generating transcript
                    transcript_prompt = (
                        f"Can you generate the transcript between the interviewer and interviewee for the role of {job} "
                        f"such that the candidate got {perf} and the reason for selection should be {reason} "
                        f"and the description of the job looks like {desc}"
                        "Don't generate any text apart from the required thing"
                    )
                    response = client.chat.completions.create(
                        model="meta-llama/Llama-3.3-70B-Instruct-Turbo",
                        messages=[{"role": "user", "content": transcript_prompt}],
                    )
                    transcript = response.choices[0].message.content

                    # Prompt for generating resume
                    resume_prompt = (
                        f"Can you generate a resume for a candidate applying for the role of {job} who was selected? "
                        f"The selection reason is {reason}, and the job description looks like: {desc}. "
                        f"The resume should reflect a typical candidate profile for this job and the selection reason."
                        "Don't generate any text apart from the required thing"
                    )
                    resume_response = client.chat.completions.create(
                        model="meta-llama/Llama-3.3-70B-Instruct-Turbo",
                        messages=[{"role": "user", "content": resume_prompt}],
                    )
                    resume = resume_response.choices[0].message.content

                    row = {'Role': job,'Transcript': transcript,'Resume': resume,'Performance(select/reject)': perf,'Reason for decision': reason,'Job Description': desc}
                    df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)

                    # Print and wait to prevent exceeding per-second requests
                    print(f"Processed selection for {job} with reason: {reason}")
                    time.sleep(1)  # Sleep for 1 second to limit request rate

## Saving the Generated Data

df.to_csv("KasaPavan_SyntheticData.csv")

print(df['Transcript'][1])