# -*- coding: utf-8 -*-
"""ResumeScreening.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I0p0Yzq3fh2knqmxpDSpreyZPgdpQ4Cm
"""



import re
import spacy
import joblib
import pandas as pd
import numpy as np
from datetime import datetime
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from textblob import TextBlob
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from transformers import BertTokenizer, BertModel
import torch
import nltk

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans



"""# ***Extracting Some features from Resume and Job Description and Deriving one metric , then Using Kmeans Clustering for Resume Screening***"""

new_df = pd.read_excel('/content/processed_data.xlsx')

new_df.head()

new_df.columns

req_df = new_df[['Role','decision','Resume_sentiment' ,'Resume_words' , 'Years_Experience' , 'resume_jd_similarity' ,  'resume_sentence_count' , 'resume_avg_word_length' ,  'skill_match_count' , 'university_education_count']]

req_df.head()

req_df.columns

# Drop irrelevant columns (e.g., Role and decision)
df = req_df.drop(columns=['Role', 'decision'], errors='ignore')

# Normalize numerical features
scaler = MinMaxScaler()
normalized_features = scaler.fit_transform(df)

# Combine normalized features into a DataFrame
normalized_df = pd.DataFrame(normalized_features, columns=df.columns)

# Calculate a "Fit Score" using a weighted combination of features
# Assigning arbitrary weights (tune as needed)
weights = {
    'Resume_sentiment': 0.2,
    'Resume_words': 0.1,
    'Years_Experience': 0.3,
    'resume_jd_similarity': 0.3,
    'resume_sentence_count': 0.05,
    'resume_avg_word_length': 0.05,
    'skill_match_count': 0.4,
    'university_education_count': 0.2
}

# Create a weighted score
normalized_df['Fit_Score'] = sum(normalized_df[col] * weight for col, weight in weights.items() if col in normalized_df)

# Set thresholds using clustering (optional)
kmeans = KMeans(n_clusters=3, random_state=42)
normalized_df['Suitability_Cluster'] = kmeans.fit_predict(normalized_df[['Fit_Score']])

# Map clusters to meaningful labels (e.g., 0 = Not Suitable, 1 = Moderately Suitable, 2 = Highly Suitable)
cluster_mapping = {0: "Not Suitable", 1: "Moderately Suitable", 2: "Highly Suitable"}
normalized_df['Suitability_Label'] = normalized_df['Suitability_Cluster'].map(cluster_mapping)

# Display results
print(normalized_df[['Fit_Score', 'Suitability_Label']].head())

# Save results to a new CSV
normalized_df.to_csv('resume_screening_results.csv', index=False)

normalized_df.Suitability_Label.value_counts()

normalized_df['Role'] = req_df['Role']
normalized_df['decision'] = req_df['decision']

normalized_df.head()

bins = [0,0.1, 0.2, 0.4, 0.6, 0.8, 1]
labels = ['0-0.1','0.1-0.2', '0.2-0.4', '0.4-0.6', '0.6-0.8', '0.8-1']

# Create a new column with binned values
normalized_df['similarity_bin'] = pd.cut(normalized_df['Fit_Score'], bins=bins, labels=labels, include_lowest=True)



"""# ***Using Normal Resume and Job Description Similarity for Resume Screening***"""

# Use a single TfidfVectorizer for both Resume and Job Description
tfidf_vectorizer4 = TfidfVectorizer()
combined_text = new_df['Resume_processed'] + " " + new_df['Job_Description_processed']
tfidf_vectorizer4.fit(combined_text)

# Split the combined TF-IDF matrix back into Resume and Job Description matrices
resume_matrix = tfidf_vectorizer4.transform(new_df['Resume_processed'])
jobDescription_matrix = tfidf_vectorizer4.transform(new_df['Job_Description_processed'])

# Calculate row-wise cosine similarity
cos_rjd = []
for i in range(new_df.shape[0]):
    cos_rjd.append(cosine_similarity(resume_matrix[i], jobDescription_matrix[i])[0][0])

new_df['resume_job_similarity'] = cos_rjd

## Here , Except for data engineer , software engineer , and ui engineer for all other roles for rejection the similarity between job description and resume is higher

bins = [0,0.1, 0.2, 0.4, 0.6, 0.8, 1]
labels = ['0-0.1','0.1-0.2', '0.2-0.4', '0.4-0.6', '0.6-0.8', '0.8-1']

# Create a new column with binned values
new_df['similarity_bin'] = pd.cut(new_df['resume_job_similarity'], bins=bins, labels=labels, include_lowest=True)

new_df.groupby(['similarity_bin', 'decision','Role']).size().unstack(fill_value=0)

"""## From this For Tf-Idf embeddings we can see that,

1. For Data Engineer We can see that , if similarity between Job description and Resume is less than the 0.2 then we are able to reject the application
2. Same for Product Manager , software Engineer , ui enginner, data analyst , data scientist
3. For ui designer 0.4 will be a good threshold for rejection
"""

## Using Bert Embeddings

df = pd.read_excel('/content/bert_embeddings.xlsx')

df_expanded = df.copy()

tra_emb = [col for col in df_expanded.columns if col.startswith('trans_emb_')]
resume_emb = [col for col in df_expanded.columns if col.startswith('resume_emb_')]
jd_emb = [col for col in df_expanded.columns if col.startswith('jd_emb_')]

tra_emb = df_expanded[tra_emb].values
resume_emb = df_expanded[resume_emb].values
jd_emb = df_expanded[jd_emb].values

jd_res_simi = []
for jd , res in zip(jd_emb , resume_emb):
    jd_res_simi.append(cosine_similarity([jd],[res])[0][0])

df_expanded['Bert_ResJobDesc_Similarity'] = jd_res_simi



bins = [ 0.95, 0.955, 0.96, 0.965, 0.97, 0.975, 0.98, 0.985, 0.99, 0.995, 1]
labels =['0.95-0.955', '0.955-0.96', '0.96-0.965', '0.965-0.97', '0.97-0.975', '0.975-0.98', '0.98-0.985', '0.985-0.99', '0.99-0.995', '0.995-1']

# Create a new column with binned values
df_expanded['similarity_bin'] = pd.cut(df_expanded['Bert_ResJobDesc_Similarity'], bins=bins, labels=labels, include_lowest=True)

df_expanded.groupby(['similarity_bin', 'decision','Role']).size().unstack(fill_value=0)

"""#### Here using the BERT embeddings we can see that, if similarity score falls between 0.95-0.96, we can reject for almost all positions"""